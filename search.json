[
  {
    "objectID": "todos.html",
    "href": "todos.html",
    "title": "TODOs",
    "section": "",
    "text": "Clean up, including headers, footnotes, Boxes, Tables captions, Figures captions,\n\nIndex_1.qmd\nIntroduction_1.qmd (rmk)\nStage_1.qmd\nStage_2.qmd\nStage_3.qmd\nStage_4.qmd\n\nComplete the About page with everything needed\nChecklist to be ticked off\n\nprinted?\nas website?\n\n\n\n\n\n\nDo we need a unified List of Tables, Figures and Boxes?\nFormat of website needs to be decided\nPrint Versions?"
  },
  {
    "objectID": "todos.html#content-of-website",
    "href": "todos.html#content-of-website",
    "title": "TODOs",
    "section": "",
    "text": "Clean up, including headers, footnotes, Boxes, Tables captions, Figures captions,\n\nIndex_1.qmd\nIntroduction_1.qmd (rmk)\nStage_1.qmd\nStage_2.qmd\nStage_3.qmd\nStage_4.qmd\n\nComplete the About page with everything needed\nChecklist to be ticked off\n\nprinted?\nas website?"
  },
  {
    "objectID": "todos.html#format-of-website",
    "href": "todos.html#format-of-website",
    "title": "TODOs",
    "section": "",
    "text": "Do we need a unified List of Tables, Figures and Boxes?\nFormat of website needs to be decided\nPrint Versions?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Stage_1.html",
    "href": "Stage_1.html",
    "title": "Requests and scope",
    "section": "",
    "text": "An assessment begins upon receiving a request from a government (or when receiving inputs and suggestions from a stakeholder), which is then considered by the Plenary. The procedure for receiving and prioritizing requests represents the first stage in defining a new work programme. It has taken place once so far, in order to define the first work programme of IPBES, for 2014-2018. A new call for requests will take place towards the end of each work programme. The first set of requests led the Plenary to carry out a set of thematic, methodological and regional assessments, as well as a global assessment.\nNote: The mechanism described in the following section was used for the production of the first work programme. The Plenary may decide, when building the second IPBES work programme, to make a number of adjustments based on lessons learnt. This means that some of the steps described below might be modified as lessons are learnt."
  },
  {
    "objectID": "Stage_1.html#footnotes",
    "href": "Stage_1.html#footnotes",
    "title": "Requests and scope",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Bureau may consider requests after the deadline on an extraordinary basis.↩︎"
  },
  {
    "objectID": "Stage_3.html",
    "href": "Stage_3.html",
    "title": "Stage 3: Approval/acceptance",
    "section": "",
    "text": "The draft SPM and chapters are presented by the secretariat to the Plenary for its consideration. Governments are given the opportunity to submit written comments to the secretariat prior to the Plenary. These comments assist the assessment experts in preparing for the Plenary but do not result in a revised draft."
  },
  {
    "objectID": "Stage_3.html#footnotes",
    "href": "Stage_3.html#footnotes",
    "title": "Stage 3: Approval/acceptance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nContact groups co-chaired by Bureau members may be established by the Plenary to address issues raised and to revise the SPMs accordingly for further consideration by the Plenary (IPBES/4/19, p.7).↩︎\nDecision IPBES-2/3: Procedures for the preparation of the Platform’s deliverables. https://www.ipbes.net/document-library-catalogue/decision-ipbes-23↩︎"
  },
  {
    "objectID": "Introduction.html#what-is-an-ipbes-assessment",
    "href": "Introduction.html#what-is-an-ipbes-assessment",
    "title": "The IPBES Guide on the production of assessments",
    "section": "What is an IPBES assessment?",
    "text": "What is an IPBES assessment?\nIPBES assessments synthesize and critically evaluate peer-reviewed scientific literature, grey literature and other available knowledge, such as indigenous and local knowledge. The assessments include a review and synthesis, as well as an analysis and an expert judgement of available knowledge. Experts are guided in this work by a conceptual framework outlining the interaction between people and nature and by guidance on the conceptualization of values of biodiversity and nature's contributions to people. An assessment does not involve the undertaking of new primary research but may include re‑analysis of data and models to address specific questions. Findings should be policy relevant but not policy-prescriptive. They could feed into and be guided by the work on policy support tools and methodologies, including its catalogue.[^5]\nIPBES assessments need to be credible, legitimate and relevant. They typically:\n\nInvolve governments and other stakeholders in the initiation, scoping, review and adoption of the assessment reports (this involvement promotes credibility, legitimacy and relevance at policy level);\nOperate through an open and transparent process, run by a group of experts that has a balance of disciplines, geography and gender, and who use agreed conceptual frameworks, methodologies and support tools, as well as are subject to independent peer review (this process promotes credibility, legitimacy and relevance at scientific level); and\nPresent findings and knowledge gaps that are policy relevant but not policy prescriptive, where the level of confidence and the range of available views are presented in an unbiased way (this approach promotes relevance at both scientific and policy level).\n\nIPBES assessments not only focus on what is known, but also on what is currently uncertain. Assessments play an important role in guiding policy through identifying areas of broad scientific agreement as well as areas of scientific uncertainty that may need further research.\nIPBES may undertake different types of assessments at sub-regional, regional and global levels. It also encourages and helps catalyse other assessments at lower scales, such as those with a local, national and a more limited sub-regional scope. IPBES is currently engaged in, has undertaken or has planned to undertake the following:\n\nGlobal assessments to assess biodiversity and ecosystem services and their interlinkages at the global scales. The global assessments will draw upon the work undertaken by the regional assessments.\nRegional assessments to assess biodiversity and ecosystem services and their interlinkages at the regional and, as necessary, sub-regional levels (e.g., Africa, the Americas, Asia and the Pacific, and Europe and Central Asia). Regional assessments will provide the building blocks for the global assessments.\nThematic assessments to assess a particular theme at an appropriate scale or a new topic (e.g., assessments of pollinators, pollination and food production, land degradation and restoration, invasive alien species and their control, and sustainable use).\nMethodological assessments to assess the availability and use of methods in relation with a specific topic (e.g., values, scenarios and models) so that these methods can then be used in IPBES assessments and other activities."
  },
  {
    "objectID": "Introduction.html#what-are-the-operating-principles-functions-and-rules-followed-by-ipbes",
    "href": "Introduction.html#what-are-the-operating-principles-functions-and-rules-followed-by-ipbes",
    "title": "The IPBES Guide on the production of assessments",
    "section": "What are the operating principles, functions and rules followed by IPBES?",
    "text": "What are the operating principles, functions and rules followed by IPBES?\nIPBES is defined by a set of operating principles and functions, and is implemented thanks to institutional arrangements, procedure programmes and other resources, as set out in Box 1.1.\nThe assessment relies on financial and in-kind contributions from governments, experts and partners according to the institutional arrangements and agreed norms.\n\n\nBox 1.1: IPBES at a glance\n\nObjective:\nTo strengthen the science-policy interface for biodiversity and ecosystem services for the conservation and sustainable use of biodiversity, long-term human well-being and sustainable development.\nOperating principles:\nIPBES addresses terrestrial, marine and inland water biodiversity and ecosystem services and their interactions, ensuring the Platform’s credibility, relevance and legitimacy, as well as promoting its independence.\nThe Principles further include: facilitating an interdisciplinary and multidisciplinary approach; engaging with different knowledge systems, including indigenous and local knowledge; recognizing the need for gender equity in its work; ensuring full and effective participation of developing countries; ensuring the full use of knowledge gained at all spatial scales, from local to global; integrating capacity-building into all relevant aspects of its work; and promoting a collaborative approach which builds on existing initiatives and experiences.\nFunctions - Catalyse the generation of new knowledge to address gaps in knowledge identified in IPBES assessments. - Deliver global, regional, sub-regional and thematic assessments, and promote and facilitate assessments at the national level at the same time. | - The development and use of policy support tools and methodologies so that assessment results can be more effectively applied. - Identify and prioritize capacity building needs for improving the science-policy interface at appropriate levels, and provide, call for and facilitate access to the necessary resources for addressing the highest priority needs directly relating to its activities.\nInstitutional arrangements\n\n\n\nMultilevel structure diagram\n\n\nProcedures, programmes and other resources - Rules of procedure for the Plenary - Financial procedures - Procedure for receiving and prioritizing requests put to the platform - The work programme - Conceptual framework - Procedures for the preparation of Platform deliverables - Guidelines on how to carry out work in the context of IPBES (assessments, scenarios, valuation) - Catalogues (of assessments and policy support tools) - Information and data management plan - Strategic partnerships - Stakeholder engagement and outreach"
  },
  {
    "objectID": "Introduction.html#how-to-use-this-guide",
    "href": "Introduction.html#how-to-use-this-guide",
    "title": "The IPBES Guide on the production of assessments",
    "section": "How to use this Guide",
    "text": "How to use this Guide\nThis Assessment Guide is aimed at those who are involved in an IPBES assessment, such as co‑chairs, authors, review editors and members of the technical support units (TSUs). The core part of the Guide sets out the four stages of an IPBES assessment and their different steps. It then sets out the roles and responsibilities of the different actors involved in an assessment. Additionally, guidance is provided on developing an SPM and on using confidence terms. The Guide is supported by a series of modules (see Box 1.2), which contain further information for those involved in IPBES assessments, and other resources such as webinars, e-learning modules, and the IPBES Catalogue for Policy Support Tools and Methodologies (see Box 1.3).\n\n\nBox 1.2: Modules of the IPBES Assessment Guide\n\n\nA: Addressing conceptual issues – the IPBES Conceptual Framework, IPBES terrestrial and aquatic units of analysis, and the IPBES Classification of Nature’s Contributions to People.\n\nB: Use of methodologies in assessments – conceptualising values, scenarios and models, and indigenous and local knowledge systems.\n\nC: Identifying and assessing data, information and knowledge resources and gaps – data and indicators.\n\nD: Enhancing the utility of assessments for decision-makers and practitioners – policy support tools and methodologies, methodological guidance for assessing policy support tools and methodologies/instruments within an IPBES assessment.\n\nE: Approaches to undertaking a government review.\n\nF: Strengthening capacities in the science-policy interface – how to address capacity building in assessments.\n\nG: Undertaking a national ecosystem assessment.\n\nH: IPBES core glossary.\n\n\nWithin each of the modules, assessment practitioners can find information around concepts, recommended practical steps and key resources, as well as guidelines, plans, strategies and approaches. When the modules become available, they can be downloaded individually from: https://www.ipbes.net/deliverables/2a-assessment-integration. This Guide, including the supporting modules, is considered a living document. It will be updated periodically to reflect the ongoing work on the Platform with new modules and sub-modules being added as required. Therefore, users are recommended to always ensure that they have the latest version of the Guide, which is downloadable from the IPBES website.\n\n\nBox 1.3: Other key IPBES resources\n\n\nIPBES e-learning modules: These cover different aspects of assessments and support the development of capacity. These can be accessed from the IPBES website: https://www.ipbes.net/e-learning\nIPBES webinar series: This webinar series covers different aspects of the assessment process, as well as the assessments themselves. Webinars can be downloaded from the IPBES website: https://www.ipbes.net/webinars\nIPBES Guide for Conceptualising Values: This Guide contains further information on the identification and conceptualization of different values and complements the sub-module of the Assessment Guide. It can be accessed from the IPBES website: www.ipbes.net/guidance-and-conceptual-framework (IPBES/3/INF/7).[^6]\nIPBES Catalogue for Policy Support Tools and Methodologies: This contains information regarding a range of policy support tools and methodologies, and policy instruments. The IPBES Catalogue of Assessments brings together information on and experiences gained from undertaking assessments of biodiversity and ecosystem services, from the global to sub-national scale, and forms a component of this Catalogue. It can be accessed from the IPBES website: https://www.ipbes.net/policy-support"
  },
  {
    "objectID": "Final text - IPBES guide for assessments - M.NS18.6.html",
    "href": "Final text - IPBES guide for assessments - M.NS18.6.html",
    "title": "Final text - IPBES guide for assessments - M.NS18.6",
    "section": "",
    "text": "The IPBES Guide on the production of assessments\nCore version"
  },
  {
    "objectID": "Final text - IPBES guide for assessments - M.NS18.6.html#footnotes",
    "href": "Final text - IPBES guide for assessments - M.NS18.6.html#footnotes",
    "title": "Final text - IPBES guide for assessments - M.NS18.6",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\nThese ecosystem services – many times enhanced by human efforts – include, for example, the provision of food and fibre; the production of oxygen and soil; the regulation of diseases and climate; and contributions to human innovation, culture and spirituality. Within IPBES, the term “nature’s contributions to people” is used.\n\n↩︎\n\nFor further information, see the following document on the ‘Functions, operating principles and institutional arrangements of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services’: https://www.ipbes.net/sites/default/files/downloads/Functions%20operating%20principles%20and%20institutional%20arrangements%20of%20IPBES_2012.pdf\n\n↩︎\n\nThese disciplines currently include the thematic assessment of pollinators, pollination and food production and methodological assessment of scenario analysis and modelling ; the thematic assessment on land degradation and restoration; regional assessments of biodiversity and ecosystem services for Africa, the Americas, Asia and the Pacific, and Europe and Central Asia; and the upcoming global assessment of biodiversity and ecosystem services.\n\n↩︎\n\nThis Guide is one of the deliverables from the first IPBES programme of work 2014-2018 (deliverable 2 (a)). Agreed in December 2013, the programme laid the groundwork for a number of deliverables, including the development of guidance materials and the scoping and completion of thematic and regional assessments.\n\n↩︎\nFor the catalogue of policy support tools and methodologies see: https://www.ipbes.net/policy-support↩︎\nIPBES/3/INF/7: Preliminary guide regarding diverse conceptualization of multiple values of nature and its benefits, including biodiversity and ecosystem functions and services (deliverable 3 (d)).\nhttps://www.ipbes.net/document-library-catalogue/ipbes3inf7↩︎\n\nThe Bureau may consider requests after the deadline on an extraordinary basis.\n\n↩︎\nIPBES/2/17: Final report and decisions of the second session of the Plenary of IPBES. https://www.ipbes.net/document-library-catalogue/ipbes217↩︎\nDecision IPBES-4/3: Rules and procedures for the operation of the platform. https://www.ipbes.net/document-library-catalogue/decision-ipbes-43↩︎\nIPBES/4/19: IPBES-4 Meeting Report. Page 107. https://www.ipbes.net/document-library-catalogue/ipbes419↩︎\nDecision IPBES-4/3: Rules and procedures for the operation of the platform. Annex I. https://www.ipbes.net/document-library-catalogue/decision-ipbes-43↩︎\nDecision IPBES-4/3: Rules and prodecures for the operation of the Platform. https://www.ipbes.net/document-library-catalogue/decision-ipbes-43↩︎\nhttps://www.ipbes.net/conflict-interest-policy-implementation-procedures↩︎\nhttps://www.ipbes.net/sites/default/files/downloads/pdf/SPM_Deliverable_3c.pdf↩︎\nThe policy support tools and methodologies guidance is under development and may modify this section.↩︎\nIPBES/6/INF/16:\nInformation on work related to policy support tools and\nmethodologieshttps://www.ipbes.net/system/tdf/ipbes-6-inf-16_-_re-issued.pdf?file=1&type=node&id=16529↩︎\nValidation of the Platform’s reports is a process by which the MEP and Bureau provide their endorsement that the processes for the preparation of the Platform reports have been duly followed (IPBES/3/18, p.75).↩︎\nContact groups co-chaired by Bureau members may be established by the Plenary to address issues raised and to revise the SPMs accordingly for further consideration by the Plenary (IPBES/4/19, p.7).↩︎\nDecision IPBES-2/3: Procedures for the preparation of the Platform’s deliverables. https://www.ipbes.net/document-library-catalogue/decision-ipbes-23↩︎\nIPBES-3/4 : Communication, stakeholder engagement and strategic partnerships. https://www.ipbes.net/document-library-catalogue/decision-ipbes-34↩︎\nIPBES/3/18: Meeting report. https://www.ipbes.net/document-library-catalogue/ipbes318↩︎\nSee the Bureau report here: https://www.ipbes.net/sites/default/files/downloads/pdf/IPBES-Bureau_7_10%20COM.pdf↩︎"
  },
  {
    "objectID": "Complete.html",
    "href": "Complete.html",
    "title": "The complete IPBES Guide on the production of assessments",
    "section": "",
    "text": "Is this needed?"
  },
  {
    "objectID": "Stage_4.html",
    "href": "Stage_4.html",
    "title": "Stage 4: Use of the assessment findings",
    "section": "",
    "text": "The release of the assessment report, including the front matter (preface), the SPM, the chapters and the back matter (annexes), is supported by a communication strategy. The communication strategy will be developed by the assessment management committee and approved by the Bureau. The aim of the communication strategy is to ensure that the assessment results are appropriately communicated and that they reach the target audiences.\nThe communication strategy may include the following steps:\n\nSelection and hiring of a media consultants to assist with the implementation of the strategy.\nIdentification of the main target audiences related to the assessment.\nFinalization of the communication strategy for the assessment with the media consultant, taking into account the IPBES communication, outreach and stakeholder engagement strategy and the needs of the relevant target audiences and stakeholders.\nCommunication ahead of the Plenary session where the SPM will be accepted.\nPreparation of press releases.\nPreparation of other media materials (including press kits, mini videos explaining the SPM content and PowerPoint presentations on the outcomes of the assessment).\nMobilization of all partners and stakeholders to help promote the assessment reports and expand their overall reach and impact.\nWebinar with key journalists ahead of the Plenary.\nOutreach with social media.\nMedia training for IPBES authors, as well as selected MEP, Bureau and secretariat staff.\nCommunication during the Plenary.\nPress conference to announce the approval of the SPM.\nInterviews with press, TV and radio in response to requests.\nCommunication after the Plenary.\nPublication of printed versions of the SPM and technical reports.\nAdditional press conferences as appropriate.\nMedia monitoring and follow-up.\nEngagement over the course of the year with different audiences and stakeholders following the approval of the SPM, including conference and events.\n\nIPBES uses embargoed releases and interviews prior to the launch of the SPM as a means to ensure a disciplined approach to the dissemination of its key messages and findings. This approach is implemented after the approval of the SPMs by member States.\nThe Platform’s key strategic objectives at the launch of an assessment, which is a period of heightened activity, are first to maintain vigorous, accurate and sustained press coverage; second, to coordinate and control messaging that is kept strictly within the bounds set for the Platform’s reports, namely that they should be policy relevant, not policy prescriptive; and third, to meet the requests made by end users – policymakers and scientific and technical experts in government and the private sector in particular – for the conduct of seminars, briefings and meetings.1\nCommunicating the results of the Platform’s assessments will be a challenging task because of the range and complexity of scientific issues and the increasing need to reach audiences beyond scientists and governments. With the help of a communications consulting firm, clear messages can be crafted for different audiences. Furthermore, trained science writers can translate technical language into text suitable for mass communication or can design web pages that explain scientific concepts to audiences without misconstruing or distorting the evidence underpinning those concepts.2\nThe presentation of the findings of the assessment reports, in particular the launch of the Pollination Assessment, led to very intense press activity with articles appearing on all major newswires (Reuters, AFP, EFE, etc.), 1,200 online news articles in 25 languages across 80 countries, plus numerous articles in print newspapers (e.g., NY times) and abundant radio coverage. “Jeopardy”, a long-running US TV game show with an average daily audience of nine million, also included a question based on the IPBES report in its April 26, 2016 broadcast (category: “Science Update”). A table summarizing all of these press articles is included on the IPBES website (www.ipbes.net/article/ipbes-pollinationreport-media-coverage).3\nOther activities in addition to the communication strategy can be undertaken to encourage the use of the key findings of the assessment. These include:\n\nWorking with key partners on the use of findings, such as the parties and observers under relevant multilateral agreements;\nMaking knowledge and data gaps identified within the assessments available to the scientific community and research funding agencies (through the use of the knowledge catalysis function of IPBES) in order to generate further research, monitoring and modelling;\nWorking with countries, through capacity building activities, to implement mechanisms which will help to leverage further impacts of IPBES products, such as through national platforms and national assessments;\nCapturing information on policy support tools in IPBES assessments and including it in the IPBES Catalogue to allow users to search and access the tools; and\nPreparing a number of scientific publications on various aspects of the reports, undertaken by experts in the published assessments, which will be listed on the IPBES website."
  },
  {
    "objectID": "Stage_4.html#footnotes",
    "href": "Stage_4.html#footnotes",
    "title": "Stage 4: Use of the assessment findings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIPBES-3/4 : Communication, stakeholder engagement and strategic partnerships. https://www.ipbes.net/document-library-catalogue/decision-ipbes-34↩︎\nIPBES/3/18: Meeting report. https://www.ipbes.net/document-library-catalogue/ipbes318↩︎\nSee the Bureau report here: https://www.ipbes.net/sites/default/files/downloads/pdf/IPBES-Bureau_7_10%20COM.pdf↩︎"
  },
  {
    "objectID": "Stage_2.html",
    "href": "Stage_2.html",
    "title": "Expert evaluation of the state of knowledge",
    "section": "",
    "text": "The Rules of Procedure for IPBES set out the nomination process for the different roles within an IPBES assessment (see IPBES/2/17 and decision IPBES-4/3)1 and are summarized in Table 1. The chair of IPBES, following the Plenary which requested the undertaking of an assessment, issues a call for nominations, explaining some of the requirements, particularly in terms of disciplines to cover all chapters of the assessment. Governments and observers are invited to nominate independent experts and fellows. From the nominations received, the MEP will select the report co-chairs, coordinating lead authors (CLA), lead authors (LA) and review editors (RE). The MEP will take the following into consideration when making these selections:\n\n80% of the selected experts should be nominated by governments and 20% by an organization.\nThe selection should reflect a range of scientific, technical and socio-economic views and expertise (e.g., natural and social sciences, scholars from the humanities, knowledge holders and experts in indigenous and local knowledge (ILK)).\nThe selection should have a good geographic representation, with appropriate representation of experts from developing and developed countries and countries with economies in transition.\nThe diversity of knowledge systems (including indigenous and local knowledge) should be represented.\nThe selection should aim at reaching gender balance.\n\nWhile every effort should be made to engage experts on the author teams from the relevant regions, with regards to chapters or assessments that deal with those specific regions, experts from other regions can be engaged if they can provide an important contribution to the assessment. If gaps in geographical, gender and expertise balances are identified, the co-chairs of the assessments, together with their respective CLAs, can identify potential additional experts to fill in these gaps. These experts will then be retroactively nominated following the approved procedure for filling gaps among groups of experts approved by the fourth session of the Plenary (see IPBES/4/19 and decision IPBES-4/3).2\nMEP or Bureau members that would like to participate as an expert in an assessment can be nominated for such a role, but they will have to resign from their duties as an MEP or Bureau member when accepting the new role.\nThe co-chairs, CLAs, LAs, REs, Fellows and Contributing Authors (CAs) have different responsibilities within a particular IPBES assessment. Each role in an Assessments not only has a specific nomination process but also has different responsibilities within a particular IPBES assessment (see Table 2.1). These roles are further described in Table 2.2.\nTable 2.1 Nomination and selection processes for different roles in assessments\n\n\n\n\n\n\n\nRole in assessment\nNomination and selection process\n\n\n\n\nThe Management Committees\nThe management committees for the assessments consist of the co-chairs, appointed members of the MEP and Bureau, as well as representatives from the responsible technical support unit and secretariat.\n\n\nAssessment co-chairs\nGovernments, the scientific community and other stakeholders are able to nominate appropriate experts for the roles of co-chairs, CLAs, LAs and REs in response to requests from the Chair of IPBES.\nIn addition to a call for nominations, members of the MEP and Bureau will contribute, as necessary, to identify relevant experts with an appropriate diversity of expertise and disciplines, gender balance and representation from indigenous and local knowledge (ILK) holders to ensure appropriate representation from developing and developed countries and countries with economies in transition. If the pool of original nominations is not balanced enough, additional nominations can be initiated by the procedure for filling gaps among groups of experts (decision IPBES-4/3).3\nNominations will be compiled in lists that are made available to all Platform members and other stakeholders, and will be maintained by the Platform secretariat. Experts with the most relevant knowledge, expertise and experience may only be chosen once an assessment topic has been fully scoped.\nEvery effort should not only be made to engage experts from relevant regions on the author teams for chapters that deal with specific regions, but experts from countries outside the region should also be engaged when they can provide an important contribution to the assessment.\nThe nomination process will follow these steps:\n\nNominees will be invited to fill out an application form and attach their curriculum vitae through the dedicated web portal. At the same time, nominees are asked to fill in a conflict of interest form.\nThe application form will automatically be sent to the nominating government or organization (nominator) indicated by the nominees with an email, which will provide a link to a nomination form inviting the nominators to approve and submit their nominations.\nNominators and nominees will receive an acknowledgement message once the nomination form confirming the nomination is submitted.\n\n\n\nCoordinating Lead Authors (CLA)\n\n\nLead Author (LA)\n\n\nReview Editor (RE)\n\n\nFellow\nThe nomination process is handled by the TSU for capacity building and is made by the fellow’s home institution. A call for nominations is made by the secretariat and utilizes an online process through the IPBES website for the submission of applications. The selection of fellows is done by the management committee.\n\n\nExpert Reviewer\nExpert reviewers are self-selected and register through the IPBES website following a call for expert reviewers by the secretariat.\n\n\nContributing Author (CA)\nThe lead authors and coordinating lead authors are selected by the MEP and may enlist other experts as contributing authors to assist with the work.\n\n\nManagement Committee\nThere is no nomination process for the management committee as it consists of the co-chairs, a selection of MEP and Bureau members, the TSU and the secretariat.\n\n\nTechnical Support Unit (TSU)\nOffers to host a TSU for an assessment are made to the Bureau. The Bureau will discuss the offers made and select a TSU.\n\n\nThe IPBES secretariat\nSupports the Bureau, MEP and management committees in overseeing the production of the assessment report and the provision of support by the TSU, as well as stores and provides access to assessment related materials that are not publicly available. Other key roles include supporting the Plenary, interacting with governments and ensuring that governments and other stakeholders receive all relevant documents.\n\n\n\nTable 2.2 Who is who in an IPBES assessment: Roles and responsibilities\n\n\n\n\n\n\n\n\nRole\nResponsibilities in the assessment\nAdvice for playing this role\n\n\n\n\nThe Plenary\nInitiates calls for requests, scoping and assessments, as well as approves the SPM and accepts the assessment chapters.\n\n\n\nThe Bureau\nOversees the policy and administrative aspects of the scoping process and the assessment process, including the preparations of the SPM, takes part in the management committee, and verifies the final draft report.\n\n\n\nThe Multi disciplinary Expert Panel (MEP)\nOversees the scientific and technical aspects of the scoping process and the assessment process, selects nominated experts, takes part in the management committee, and verifies the final draft report.\n\n\n\nThe Management Committee\nSupports the co-chairs and assists the Bureau, MEP and the secretariat in overseeing assessment processes, including in the filling of expertise gaps and in handling non-performing authors.\nThe management committees of the assessments consist of the co‑chairs of the assessment, appointed members of the MEP and Bureau, and representatives of the responsible technical support unit and secretariat. The management committee is chaired by the co-chairs of the relevant assessment and is responsible for supporting the co‑chairs of the relevant assessment in the day-to-day operations required for the implementation of the respective deliverable, where the substance of the matter to be addressed does not warrant alerting the MEP, Bureau or other entity responsible according to the IPBES procedures.\nThe management committee stays up to date with all developments of the assessment processes and also ensures that the processes adhere to the IPBES rules of procedure. Where the management committee cannot agree on an issue, or the scope of the matter to be addressed warrants a decision by the responsible body, the matter will be referred by the management committee to the responsible body.\nExamples of management committee responsibilities include:\n\nIdentifying and suggesting names of experts (CLAs, LAs and REs), to fill gaps in expertise, for MEP approval.\nEnsuring that the global, regional and thematic assessments are consistent in including/using:\n\n\n\nThe conceptual framework\nThe values guide\nThe scenarios and modelling assessment\nIndicators\nILK\nEcosystem services classification\n\n\n\nEnsuring the approaches and findings of the assessments are consistent.\nAssisting in the preparation of SPMs and presenations for the Plenary.\n\nHold regular meetings by teleconference or other appropriate means at least once every two months.\nBe up-to-date with the latest version of the assessment report\n(zero order draft, First (1st)order Draft, Second (2nd) order draft or final draft).\n\n\nAssessment\nco-chair\nThe role of co-chair is normally shared between two and sometimes three experts. An assessment co-chair’s role is to assume responsibility for overseeing the preparation of an assessment report, as well as its SPM, and ensuring that the report is completed to a high standard and addresses the key scoping questions. A co-chair is senior in their field and has experience in coordinating the work of experts. Besides overseeing the development of the assessment, the co-chair can also contribute text to one (or more) chapters.\nThe co-chair is also responsible for collaborating with the coordinating lead authors to ensure that the chapters are delivered in a timely manner and with a high standard, and that they address the key scoping questions. The co-chair will ensure that the chapters feed into each other and that their messages are not contradicting.\nThe co-chair participates in the setting of the agenda and the chairing of the author meetings. He/ she will work together with the management committee of the assessment to ensure that issues within the assessment are being solved and that the assessment is prepared according to the decisions and guidelines of IPBES. Once the assessment and SPM are finalized, co-chairs will also engage in the outreach for those deliverables.\nAssessment co-chairs are expected to contribute 25 to 30% of their time to the coordination of their dedicated assessment. They are expected to participate in each author meeting.\nGet up to speed with the IPBES rules and procedures, as well as other assessments and deliverables.\nRead other relevant assessments on biodiversity and ecosystem services (available in the Catalogue of Assessments).\nOrganize regular skype meetings with chapter CLAs to stay in touch with the development of the chapters.\nInvest in building trust amongst the authors, as well as a sense of pride and ownership of the assessment process.\nReview and check the key messages of the chapters in order to prepare the SPM.\n\n\nCoordinating Lead Author (CLA)\nA coordinating lead author’s role within an IPBES assessment is to assume overall responsibility for coordinating a chapter of the assessment report.\nCoordinating lead authors are lead authors who, in addition to their responsibilities as a lead author, have the responsibility of ensuring that the chapters of a report are completed to a high standard, are collated and delivered to the report co-chairs in a timely manner, and conform to any overall standards of style set for the document. They are thus to coordinate the work of the lead authors, fellows and contributing authors involved in their chapter to ensure the quality of the chapter as a whole.\nCoordinating lead authors also play a leading role in ensuring that any cross-cutting scientific, technical or socio-economic issues, of significance to more than one section of a report, are addressed in a complete and coherent manner and reflect the latest information available.\nCLAs coordinate the pulling out of key messages from their chapter and the writing of the executive summary of the chapter. They also contribute to the writing of the SPM.\nCLAs are expected to contribute 20% of their time to the coordination of their chapter. They are expected to participate in the author meetings and to coordinate the work of their chapter at the meeting.\nOrganize regular communication between the different LAs and fellows in your chapter.\nReview the text received and structure information to create a flowing chapter.\nPut deadlines for the author team to ensure the timely delivery of the different order drafts.\nIdentify gaps in the chapter author team and search for potential CAs to fill those gaps.\n\n\nLead Author (LA)\nThe role of a lead author is to assume the responsibility of producing designated sections or parts of chapters that respond to the work programme of the Platform, on the basis of the best scientific, technical and socio-economic information available.\nLead authors typically work in small groups that together are responsible for ensuring the various components of their sections are put together on time, are of a uniformly high quality and conform to any overall standards of style set for the document.\nThe essence of the lead authors’ role is to synthesize material drawn from the available literature and fully-justify unpublished sources, contributing authors, stakeholders and experts where appropriate.\nLead authors can identify contributing authors who can provide additional technical information or graphics on specific subjects covered in the chapter.\nLAs are expected to contribute 15% of their time to producing relevant sections and parts to their dedicated chapters. They are also expected to participate actively in the author meetings.\nActively participate in discussions within the chapter team about the content of the chapter.\nDivide tasks amongst lead authors and identify the areas that each will write about.\nGet familiarized with previous IPBES assessments to learn about the style and overall standards expected.\nCollect peer reviewed literature for the author team to use.\nWhen gaps are experienced in the chapter, consider where you could use a contributing author to fill those gaps.\n\n\nContributing Author (CA)\nA contributing author’s role is to prepare technical information in the form of text, graphs or data for inclusion by lead authors in the relevant sections or part of a chapter.\nInput from a wide range of contributors is key to the success of Platform assessments. Contributions are sometimes solicited by lead authors but spontaneous contributions are also encouraged. Contributions should be supported, as far as possible, with references from peer-reviewed and internationally available literature.\nContributing authors are responsible only for contributing to a specific part of the chapter and do not work on the chapter as a whole. They will be listed only as a contributing author if their input is included in the final report. Contributing authors are not formally nominated and also do not normally fill in the conflict of interest forms. They are not privy to all communication in the chapter team but work directly with the LA or CLA who is coordinating the CA’s technical input into the chapter.\nProvide technical information in concise and clear text or graphs.\nProvide adequate referencing from peer-reviewed materials to your contribution.\nCoordinate your input with the authors of the chapter to see where your contribution is best fitted and adapt it to the content of the overall chapter.\nKeep the confidentiality of the report in mind when being part of the author team.\n\n\nReview Editor (RE)\nReview editors are seniors in their field and may represent a range of scientific, technical and socioeconomic views. They therefore have expertise in one or more natural and/or social scientific discipline, and may represent or have expertise in indigenous and local knowledge. The review editors get involved as of the review phase of the first order draft and help the author teams to address review comments during the second and third author meeting. They also help to ensure that confidence terms are used consistently throughout the executive summary of the related chapter.\nIn general, there will be two review editors per chapter, including its executive summary. It is also possible that an assessment has one or more overall review editor that reviews the entire report, or an additional review editor that reviews the SPM. Review editors are not actively engaged in drafting reports and may not serve as reviewers for text that they have been involved in writing.\nThe review editors’ main tasks are: (i) to assist the MEP in identifying reviewers for the expert review process; (ii) ensure that all substantive expert and government review comments are afforded appropriate consideration; (iii) advise lead authors on how to handle contentious or controversial issues; and (iv) ensure that genuine controversies are adequately reflected in the text of the report concerned.\nResponsibility for the final text of the report remains with the relevant CLAs and LAs.\nReview editors must submit a written report to the MEP and, where appropriate, will be requested to attend a meeting convened by the MEP to communicate their findings from the review process and assist in finalizing summaries for policymakers and synthesis reports. The names of all review editors will be acknowledged in the reports.\nReview editors participate in the second and third author meetings.\nGet accustomed to the content of the chapter of which you are the review editor well before the second author’s meeting.\nConsider who would be a suitable candidate for performing the expert review.\nRefrain from imposing changes in the text to the author team.\nReview the responses from authors to comments received.\nBe a good sparring partner to the author team and make good judgement calls.\nBe open to different perspectives and world views.\n\n\nExternal reviewers\nExternal reviewers carry out the external review of the first and second order drafts of the assessment report and the SPM. They have to register as an expert reviewer in order to be able to comment on the accuracy and completeness of the s cientific/technical/socio-economic content and the overall s cientific/technical/socio-economic balance of the drafts. An expert reviewer evaluates the quality, validity and relevance of the assessment. The aim of an external review is to provide authors with constructive feedback that will help in preparing an assessment of the highest quality.\nExperts who are nominated by governments and observer organizations but are not selected are encouraged to contribute to the report as Externalt Reviewers.\nExternal reviewers are independent experts (i.e., experts not involved in the preparation of that particular chapter). They will be mentioned as expert reviewers in the final report.\nComment in a constructive tone.\nComment also on parts of the text that are relevant and that should stay in the text.\nBe specific and use full citations for relevant papers when providing suggestions for text revisions.\nSuggest ways to shorten text and/or display content using figures or tables.\nFocus on substantive issues (comments on spelling, text style and grammar are not needed).\nWhen reviewing the draft report, also take note of the original scoping document for the assessment.\nComments will only be accepted in English and in the given review format.\nComments are to be given within the review deadline.\n\n\nFellows\nThe IPBES fellowship programme allows early career researchers and other professionals to engage with the Platform’s activities and work alongside more experienced colleagues. Fellows are experts that are in the early stages of their careers, indicatively not older than 35 years of age or 5-10 years of experience on from obtaining their academic degree. They should be working in the area of social, economic and biological sciences, policy development, and/or indigenous and local knowledge systems.\nFellows are an integral part of the IPBES assessment chapters and collaborate with the CLAs and LAs in developing sections or parts of chapters. They receive training to gain an in-depth understanding of the IPBES assessment processes. Fellows will also be paired up with a mentor for the assessment period.\nFellows are expected to participate in the author meetings.\nCoordinate your role in the chapter with your mentor, as well as your chapter’s existing CLAs and LAs.\nGarner knowledge from other IPBES assessments.\nDo not be afraid to bring in new ideas or ask questions!\n\n\nTechnical support unit (TSU)\nThe IPBES secretariat is mandated to provide technical support to the expert working groups. Technical support needed for the development of the deliverables, including the assessments, will in principle be provided by the secretariat. In many instances however, the technical support needed exceeds the capacity of the secretariat in its planned composition and it is more cost effective when additional technical support to expert groups is provided through the establishment of technical support units.\nEach assessment has one dedicated technical support unit, normally hosted by a partner institution and consisting of a couple of technical and administrative staff members. Technical support units represent one avenue for involving regional hubs and regional or thematic centres of excellence in the work of the Platform. It can also happen that the technical support unit is hosted within the IPBES secretariat. In any case, the TSU works under the oversight of the secretariat to coordinate and administer the activities for the assessment expert group.\nSome of these main activities include:\n\nProviding guidance to the expert group to ensure that activities are delivered in accordance with the guidance of the MEP, related IPBES decisions and with the rules of procedure of the Platform.\nThe provision of logistical, technical and thematic support (through documents, communications, contacts, etc.) to experts in order to facilitate their participation in the assessment.\nSupporting the formatting and editing of the regional assessment report and the identification of plagiarism risks.\nSupporting the organization and storage of reference materials and data used in the assessment report, making assessment related material that are not publicly available accessible to reviewers, and submitting materials to the IPBES secretariat for archiving.\nSupporting the expert group in convening teleconferences, as well as putting in place the necessary teleconference services to facilitate calls.\nCollaborating with the IPBES secretariat and providing regular feedback to the secretariat on the progress of the assessment report.\n\nProvide regular updates to both the assessment teams and the secretariat on assessment developments.\nBuild relationships with your authors to facilitate the building of trust.\nStay up to date with all IPBES relevant rules of procedures and Plenary decisions.\n\n\nThe IPBES secretariat\nThe IPBES secretariat supports the Bureau, MEP and management committees in overseeing the production of the assessment report, oversees the provision of support by the TSU, and stores and provides access to assessment-related materials that are not publicly available. Other key roles include supporting the Plenary, interacting with governments, and ensuring that governments and other stakeholders receive all relevant documents.\n\n\n\n\n\n\nBox 2.1: Writing suggestions for assessment reports\n\nThe suggestions below are based on comments received during the Millennium Ecosystem Assessment peer review process:\n\nDiscuss the problems and actions first. Any necessary background can come later in an appendix or in references to other sources.\nFocus on definable measures/actions and avoid the passive voice. For example, policy professionals are likely to ignore statements like “there are reasons to believe some trends can be slowed or even reversed”. If there are some opportunities for reversal, state precisely what we believe they are to the best of our knowledge.\nStatements like “...might have enormous ramifications for health and productivity...” may seem to be strong because of the word “enormous”, but they are actually politically impotent because of the word “might.” If data are used in the assessment, what do they say about what “is” happening? What can we recommend, based on the best available knowledge, about what actions would be effective?\nA statements like “there is a long history of concern over the environmental effects of fishing in coastal habitats, but the vast scope of ecological degradation is only recently becoming apparent (citation)” is a case where something strong could be said but it is weakened by putting emphasis on the late arrival of this information and knowledge “becoming apparent”. It does not matter so much when the degradation was discovered, what matters is that it was. Cite the source and say “fishing practices are causing wide‑spread destruction”.\nDo not use value-laden, flowery or colloquial language (e.g., “sleeping dragon”, “elephant in the room”, etc.).\nStatements like “we do not yet have clear guidelines for achieving responsible, effective management of natural resources” could result in a legitimate policy response of “OK, so we’ll wait until we do”. Instead, the statement could be changed to recommend what needs to be done, such as, “if clear guidelines were developed, then…”.\nDiverse formats and modes of communication, for example participatory maps, artwork and visual imagery, will be important for working with indigenous and local knowledge.\n\nSource: Ash et al., 2010"
  },
  {
    "objectID": "Stage_2.html#why-does-our-communication-of-confidence-matter-in-ipbes-assessments",
    "href": "Stage_2.html#why-does-our-communication-of-confidence-matter-in-ipbes-assessments",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.1 Why does our communication of confidence matter in IPBES assessments?",
    "text": "6.1 Why does our communication of confidence matter in IPBES assessments?\nKnowledge and scientific data about the natural world and the influence of human activities are complex. There is a need to communicate what the assessment author teams have high confidence in as well as what requires further investigation to allow decision-makers to make informed decisions. Furthermore, by following a common approach when applying confidence terminology within an assessment, authors are able to increase consistency and transparency.\nIPBES assessments will use specific phrases known as “confidence terms” in order to ensure consistency in the communication of confidence by author teams. What confidence term is used will depend on the author team’s expert judgement on the quantity and quality of the supporting evidence and the level of scientific agreement. IPBES assessments use a four-box model of confidence (see Figure 2.5) based on evidence and agreements that give four main confidence terms: “well established” (much evidence and high agreement), “unresolved” (much evidence but low agreement), “established but incomplete” (limited evidence but good agreement) and “inconclusive” (limited or no evidence and little agreement).\nThe following guidance will discuss where confidence terms must be applied in the IPBES assessment reports, how to select the appropriate term to communicate the author team’s confidence and how to present the confidence terms in the text.\nConfidence terms should always be used in two key parts of an assessment: 1. They should be assigned to the key findings in the executive summaries of the technical chapters in an assessment report (see Box 2.2), and 2. For both key messages and key findings of the SPMs (see Box 2.3)."
  },
  {
    "objectID": "Stage_2.html#how-to-do-i-select-confidence-terms",
    "href": "Stage_2.html#how-to-do-i-select-confidence-terms",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.2 How to do I select confidence terms?",
    "text": "6.2 How to do I select confidence terms?\nOnce the author team has identified the chapter’s key messages and findings, in order to present these in the executive summary or SPM, it is mandatory to evaluate the quality and quantity of associated evidence and scientific agreement. Author teams will always be required to make qualitative assessments of confidence based on expert estimates of agreements and evidence.\nDepending on the nature of the evidence supporting the key message or finding, quantitative assessments of confidence may also be possible. Quantitative assessments of confidence are estimates of the likelihood (probability) that a well-defined outcome will occur in the future. Probabilistic estimates are based on statistical analyses of observations or model results, or both, combined with expert judgment. However, it may be that quantitative assessments of confidence are not possible in all assessments due to the nature of the evidence available.\nIt is not mandatory to apply confidence terms throughout the main text of the assessment report. However, in some parts of the main text, in areas where there are a range of views that need to be described, confidence terms may be applied where considered appropriate by the author team. In order to avoid confusing readers, in no case should the terms be used colloquially or casually. Use these terms if you have followed the recommended steps for assessing confidence."
  },
  {
    "objectID": "Stage_2.html#qualitative-assessment-of-confidence",
    "href": "Stage_2.html#qualitative-assessment-of-confidence",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.3 Qualitative assessment of confidence",
    "text": "6.3 Qualitative assessment of confidence\nThis section discusses the process and language that all author teams must apply to evaluate and communicate confidence qualitatively. The following factors should be considered to evaluate the validity of the message or finding: the type, quantity, quality and consistency of evidence (the existing peer‑reviewed literature and grey literature, etc.), as well as the level of agreement (the level of concurrence in the data, literature and amongst experts, not just across the author team). The author team’s expert judgement on the level of evidence and agreement should then be used to apply a confidence term as described in Figure 2.4:\n\nInconclusive – existing as or based on a suggestion or speculation; no or limited evidence.\nUnresolved – multiple independent studies exist but conclusions do not agree.\nEstablished but incomplete – general agreement although only a limited number of studies exist but with no comprehensive synthesis, or the studies that do exist imprecisely address the question.\nWell established – comprehensive meta-analysis or other syntheses/multiple independent studies that agree.\n\n\n\n\ncid:BEEEA707-B80E-4341-8E3B-BF017448D6DA@home\n\n\nFigure 2.4. The four-box model for the qualitative communication of confidence. Confidence increases towards the top-right corner as suggested by the increasing strength of shading.\n\nNote: ‘well-established’ can be further subdivided into ‘very well established’ and ‘virtually certain’.\nSource: IPBES, 2016.\nThe well-established box in Figure 2.4 may be further subdivided in order to give author teams the flexibility to emphasize key messages and findings that the author team have very high confidence in:\n\nVery well established – very comprehensive evidence base and very low amount of disagreement.\nVirtually certain – very robust data covering multiple temporal and spatial scales and almost no disagreement.\n\nThe qualitative confidence terms discussed in this section should not be interpreted probabilistically and are distinct from “statistical significance”.\nVirtually certain will not be used by the author teams frequently in the assessment report. The confidence terms used to communicate high confidence are intended to provide authors with the flexibility to emphasize issues that may be considered as fact by the scientific community but not by the non-scientific community (decision-makers, media and the general public). In most cases it may be appropriate to describe these findings using overwhelming evidence and agreements as statements of fact without using confidence qualifiers.\nSimilarly, inconclusive may also be used infrequently, but is intended to provide authors with the flexibility to emphasize issues that are not established in science but that are important to policymakers or might have been highlighted by a different audience.\nThe degree of confidence in findings that are conditional on other findings should be evaluated and reported separately.\nWhen evaluating the levels of evidence and agreements for a statement, it is important to standardize the use of the terms within and across the author teams and, when possible, across the assessment in order to ensure their consistent use. The use of the discussed confidence terms can be standardized by taking key messages and findings in the executive summaries and discussing, as an author team, what terms should be applied and the reasons why. When appropriate, consider using formal elicitation methods to organize and quantify the selection of confidence terms.\nBe aware of the tendency for a group to converge on an expressed view and become overconfident in it. One method to avoid this would be to ask each member of the author team to write down his or her individual assessment of the level of confidence before entering into a group discussion. If this is not done before a group discussion, important views and ranges of confidence may be inadequately discussed and assessed. It is important to recognize when individual views are adjusting as a result of group interactions and allow adequate time for such changes in viewpoint to be reviewed (Mastrandrea et al. 2010). Whichever approach is taken, traceable accounts should be produced and recorded to demonstrate how confidence was evaluated.\nIt is important to carefully consider how the sentences in the key messages and findings are structured because it will influence the clarity with which we communicate our understanding of the level of confidence. For example, sometimes the key finding combines an element that is well-established with one that is established but incomplete. In this case it can be helpful to arrange the phrasing so that the well-established element comes first and the established but incomplete element comes second, or as a separate sentence. Avoid the use of unresolved and established but incomplete where possible by writing or rewording key messages and findings in terms of what is known rather than unknown. Author teams should focus on presenting what is well-established as far as possible in order to make it clear to decision-makers what is known. Assigning confidence terms to our key findings will therefore often require that we re-write sentences, rather than simply adding the terms to existing text."
  },
  {
    "objectID": "Stage_2.html#quantitative-assessment-of-confidence",
    "href": "Stage_2.html#quantitative-assessment-of-confidence",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.4 Quantitative assessment of confidence",
    "text": "6.4 Quantitative assessment of confidence\nThis section discusses the process and language that author teams may wish to apply in order to evaluate and communicate the confidence that an outcome will occur quantitatively. Likelihood expresses a probabilistic estimate of the occurrence of a single event or of an outcome within a given range. Probabilistic estimates are based on statistical analyses of observations or model results, or both, combined with expert judgment.\nWhen sufficient probabilistic information is available, consider ranges of outcomes and their associated probabilities with attention to outcomes of potential high consequence. The author team’s expert judgement on the magnitude of the probability should then be used to apply a likelihood term (see Figure 2.5).\n\n\n\ncid:FE03DB87-0578-4829-856C-24401E083F19@home\n\n\nFigure 2.5. Likelihood scale for the quantitative communication of the probability of an outcome occurring\n\nNote: Extreme levels of probability are nested within the broader levels of “likely” and “unlikely”. Source: modified from Mastrandrea et al. 2010.\nCategories in Figure 2.5 can be considered to have nested boundaries. For example, describing an outcome as likely or very likely conveys in both cases that the probability of this outcome could fall within the range of 95% to 100% probability. However, in the case of likely, the larger range (66-100%) indicates a higher degree of confidence than very likely (90-100%). In making their expert judgement, author teams should start at about as likely as not and consider whether there is sufficient quantitative information available to assign either a likely or unlikely probability range. Only after thinking about this initial range should the author teams consider whether there is sufficient evidence to move to more extreme levels of probability.\nAuthor teams should note that using a likelihood term for a specific outcome implies that alternative outcomes have the inverse likelihood, e.g., if an outcome is likely (a range of 66-100%) then that would imply that other outcomes are unlikely (0-33% probability).\nIf the author team consider that sufficiently robust information is available with which to make a ‘best estimate’ of the probability of the occurrence of an event, then it is preferable to specify the full probability range (e.g., 90-95%) in the text without using the terms in Figure 2.5. Also, about as likely as not should not be used to communicate a lack of knowledge, only as an estimate of probability based on the available information.\nAuthor teams should be aware of the way in which key messages and findings are phrased. The way in which a statement is framed will have an effect on how it is interpreted, e.g., a 10% chance of dying is interpreted more negatively than a 90% chance of surviving. Consider reciprocal statements to avoid value-laden interpretations, e.g., report chances of both dying and surviving (Mastrandrea et al. 2010).\nFinally, author teams should try not to avoid controversial events, such as impacts or events with high consequence but extremely low probability, in their effort to achieve consensus within an author team."
  },
  {
    "objectID": "Stage_2.html#how-to-present-confidence-termspresenting-confidence-using-the-four-box-model",
    "href": "Stage_2.html#how-to-present-confidence-termspresenting-confidence-using-the-four-box-model",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.5 How to present confidence terms:presenting confidence using the four-box model",
    "text": "6.5 How to present confidence terms:presenting confidence using the four-box model\nConfidence terms are communicated as part of the key findings of an assessment. The key findings are set out in the executive summaries for each of the assessment’s chapters in the full technical report. The key findings are the facts and information drawn directly from the chapter. It is recommended that key findings should be set out as follows.\nThe first sentence of the finding should be bolded and contain a confidence term from the four-box model in italics and brackets at the end of the sentence. This first sentence is followed by two to four sentences which then supports the information contained in this first sentence. Subsequent sentences may contain confidence terms within brackets where appropriate. It is not necessary to include confidence terms with each sentence if the whole paragraph falls under the same confidence term.\nThe words that make up the four-box model and likelihood scale should not be used in the text of the assessment except when formally assigning confidence. If, for example, there was a sentence that used the word “likely” but not with the intended meaning from the likelihood scale, then the word should be replaced with another (e.g., probably)."
  },
  {
    "objectID": "Stage_2.html#presenting-confidence-using-the-likelihood-scale",
    "href": "Stage_2.html#presenting-confidence-using-the-likelihood-scale",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.6 Presenting confidence using the likelihood scale",
    "text": "6.6 Presenting confidence using the likelihood scale\nIn some instances, author teams may wish to complement the use of the well-established confidence term with a term from the likelihood scale. If terms from the likelihood scale are used then they should be incorporated into the text and italicized prior to the impact or outcome of the probability of which they are describing."
  },
  {
    "objectID": "Stage_2.html#traceability",
    "href": "Stage_2.html#traceability",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.7 Traceability",
    "text": "6.7 Traceability\nThe author team’s expert judgment of their confidence in the key messages and findings should be explained by providing a clear traceable account. A traceable account is a description in the chapter of the evaluation of the type, quantity, quality and consistency of the evidence and level of agreement that forms the basis for the given key message or finding (Mastrandrea et al. 2010). Where possible, the description should identify and discuss the sources of confidence. In order to ensure consistency in how the author teams classify sources of confidence within and across IPBES assessments, author teams should use the typology shown in Table 2.5.\nA main finding in the summary for policymakers should be readily traceable back to an executive summary main finding(s) that in turn should be readily traceable back to a section(s) of the chapter text, which should be traceable, where appropriate, to the primary literature through references.\nReferences to the relevant executive summary statement in the SPM should be included in curly brackets (e.g., {1.2}), see Box 2.2."
  },
  {
    "objectID": "Stage_2.html#summary-of-the-steps-for-applying-confidence-terms",
    "href": "Stage_2.html#summary-of-the-steps-for-applying-confidence-terms",
    "title": "Expert evaluation of the state of knowledge",
    "section": "6.8 Summary of the steps for applying confidence terms",
    "text": "6.8 Summary of the steps for applying confidence terms\nThe steps recommended for assessing and communicating confidence for executive summaries and SPMs is as follows:\n\nIdentify the chapter’s key messages and findings.\nEvaluate the supporting evidence and the level of scientific agreement.\nEstablish whether the evidence is probabilistic or not (e.g., from model predictions).\nWhere the evidence is qualitative instead or probabilistic, select a confidence term from the four-box model (see Figure 2.4) to communicate the author team’s confidence in the key message or finding.\n\nAssess the quantity and quality of evidence and the level of agreement in the scientific community.\nEstablish how confident the author team is and select the appropriate term.\n\nWhere quantitative estimates of the probability of an outcome or impact occurring are available (e.g., from model predictions), select a likelihood term from the likelihood scale (see Table 2.5) to communicate the author teams’ expert judgement of the range of the probability of occurrence.\nEnsure that there is always a ‘traceable account’ in the main text describing how the author team adopted the specific level of confidence, including the important lines of evidence used, the standard of evidence applied and the approaches to combine/reconcile multiple lines of evidence. Where specific sources of confidence are prominent for a key finding, the terms used in the left-hand column of Table 2.5 should be included in the traceable account.\nOptional: Consider using formal frameworks for assessing expert judgements for each author team.\n\nTable 2.5 Sources of low confidence\n\n\n\n\n\n\n\n\n\nSource of low confidence\nDefinition and examples\nQualities\nMeans of dealing with low confidence\n\n\n\n\nImprecise meanings of words\n(linguistic uncertainty)\nVagueness and ambiguity of terms.\nExample: When terms such as human welfare, risks, plant reproductive success and pollination deficits are central to the findings.\nReducible\nNot quantifiable\nClear, common definition of terms (IPBES Core glossary).\nProtocols as used in agent-based modelling to deal with context dependence.\n\n\nInherently un predictable systems\n(stochastic uncertainty)\nLow confidence due to the chaotic nature of complex natural, social or economic systems (sometimes known as ‘aleatory’)\nFindings that depend on weather or climate variables, or market prices, will be subject to this low confidence.\nExample: Pollination deficits and values measured at local scales.\nNot reducible\nQuantifiable\nClear communication.\nUse probabilistic approaches.\nSupport large scale, long term multi-site studies to quantify the variation over space and time to characterize the low confidence.\nEvidence synthesis.\nCapacity building for researchers and decision- makers.\n\n\nLimits of methods and data\n(scientific uncertainty)\nWhere there is insufficient data to fully answer the question due to unsatisfactory methods, statistical tools, experimental design or data quality (also referred to as epistemic uncertainty).\nExample: Impacts of pesticides on pollinator populations in the field, trends in pollinator abundance, estimations of ecosystem service delivery.\nReducible\nQuantifiable\nAcknowledge differences in conceptual frameworks (within and between knowledge systems).\nImprove experimental design.\nExpand data collection.\nSupport detailed, methodological research\nKnowledge quality assessment.\nEvidence synthesis.\nCapacity building for scientists.\n\n\nDifferences in un derstanding of the world\n(decision uncertainty)\nLow confidence that is caused by variations in subjective human judgments, beliefs, world views and conceptual frameworks (sometimes called epistemic uncertainty). In terms of policy decisions, low confidence is often due to preferences and attitudes that may vary with social and political contexts. This can mean a finding looks different in different knowledge systems that cannot easily be aligned.\nExample: Effects of organic farming look different if you take the view that wild nature beyond farmland has a higher value than farmland biodiversity, and overall food production at a large scale is more important than local impacts. There are divergent interpre tations/perceptions of well-being.\nSometimes reducible\nNot quantifiable\nAcknowledge differences in conceptual frameworks (within and between knowledge systems).\nDocument, map and integrate where possible.\nAcknowledge existence of biases.\nMulti-criteria analysis, decision support tools.\nCapacity building for decision-makers"
  },
  {
    "objectID": "Stage_2.html#what-is-a-summary-for-policymakers",
    "href": "Stage_2.html#what-is-a-summary-for-policymakers",
    "title": "Expert evaluation of the state of knowledge",
    "section": "7.1 What is a summary for policymakers?",
    "text": "7.1 What is a summary for policymakers?\nA summary for policymakers (SPM) is a short document that highlights the main messages of an assessment responding to its scoping report in a synthesized and less technical language, and tailored to the needs of policymakers. It consists of preferably 15-20 top key messages (2,000 words max) categorized under a few headings and presented without reference to the main chapters. They represent the highest level of synthesis of the assessment and may be structured differently from the set of main findings in the SPM. Each message is carefully formulated in a bolded sentence with assigned confidence levels and is supported by a paragraph of non-bolded text which substantiates the message (see Box 2.4).\n\n\nBox 2.4: Examples (extracts) of key messages from the IPBES Pollinators, Pollination and Food Production Assessment SPM\n\n6. The vast majority of pollinator species are wild, including more than 20,000 species of bees, some species of flies, butterflies, moths, wasps, beetles, thrips, birds, bats and other vertebrates. A few species of bees are widely managed, including the western honey bee (Apis mellifera), the eastern honey bee (Apis cerana), some bumble bees, some stingless bees and a few solitary bees. Beekeeping provides an important source of income for many rural livelihoods. The western honey bee is the most widespread managed pollinator in the world, and globally there are about 81 million hives producing an estimated 1.6 million tonnes of honey. |\n11. The number of managed western honey bee hives has increased globally over the last five decades, even though declines have been recorded in some European countries and North America over the same period. Seasonal colony loss of western honey bees has in recent years been high at least in some parts of the temperate Northern Hemisphere and in South Africa. Beekeepers can under some conditions, with associated economic costs, make up such losses through the splitting of managed colonies.\n1. The abundance, diversity and health of pollinators and the provision of pollination are threatened by direct drivers that generate risks to societies and ecosystems. Threats include land‑use change, intensive agricultural management and pesticide use, environmental pollution, invasive alien species, pathogens and climate change. Explicitly linking pollinator declines to individual or combinations of direct drivers is limited by data availability or complexity, yet a wealth of individual case studies worldwide suggests that these direct drivers often affect pollinators negatively.\n20. Most agricultural genetically modified organisms (GMOs) carry traits for herbicide tolerance (HT) or insect resistance (IR). Reduced weed populations are likely to accompany most herbicide-tolerant (HT) crops, diminishing food resources for pollinators. The actual consequences for the abundance and diversity of pollinators foraging in herbicide-tolerant (HT)-crop fields is unknown. Insect-resistant (IR) crops can result in the reduction of insecticide use, which varies regionally according to the prevalence of pests, the emergence of secondary outbreaks of non-target pests or primary pest resistance. If sustained, the reduction in insecticide use could reduce pressure on nontarget insects. How insect-resistant (IR) crop use and reduced pesticide use affect pollinator abundance and diversity is unknown. Risk assessments required for the approval of genetically‑modified organism (GMO) crops in most countries do not adequately address the direct sublethal effects of insect-resistant (IR) crops or the indirect effects of herbicide-tolerant (HT) and insect-resistant (IR) crops, partly because of a lack of data.\n\nThe key messages of the SPM aim to:\n\ntell a short, coherent and compelling story on the state of knowledge aimed at non‑technical decision-makers and the public;\nconvey illustrative and striking perspectives, facts and numbers from the assessment;\nset the stage for the negotiation of the SPM at the IPBES Plenary; and\nserve as a key source for media and outreach material once approved by the Plenary.\n\nThese messages are followed by a set of main findings categorized under a set of headings (approximately 10,000 words max). They tell a comprehensive story, based on the state of knowledge specific to the scoping document, and are aimed at non-technical decision-makers but with higher levels of technical yet non-jargon specificity compared to the top key messages (see Box 2.4).\nEach finding is formulated in one or two bolded sentences, substantiated and supported with statements amounting to one paragraph of text. The statements in the messages are assigned confidence levels and often start with the ones that have highest confidence. Findings can be traced back to the underlining chapter section(s) from which they are drawn.\nThe responsibility for preparing the first and revised drafts of the SPMs lies with the report co-chairs and an appropriate representation of CLAs and LAss, with a review process overseen by the MEP and Bureau.\nThe features of an SPM are that it:\n\nsets out policy relevant messages from the assessment while not being policy prescriptive, and\nbuilds on the executive summaries (key findings) from each chapter from the technical assessment report.\n\nThe development of an SPM is an iterative process as explained in the following section . You will need to make sure that the information in each chapter’s executive summary underpins the messages set out in the SPM and that the analyses in the assessment chapters support the findings in the executive summaries. Fundamentally, no information, data or knowledge should appear in the SPM if it does not appear in the technical assessment report.\nImportant points to recognize from the Pollination Assessment SPM:\n\nThere are a total of 11 key findings that are short and indicated in bold.\nEach key finding comes with a confidence language statement (in parentheses).\nFurther explanations for the key findings are provided via additional text about a paragraph in length each.\nThe total word count is around 1,400.\nEach key finding includes a list of the chapter sections that contain the relevant literature/evidence supporting it.\nSPMs of methodological assessments may contain guidance points, which are lessons from best practices for building greater understanding and strengthening approaches to and making more effective use of methodological themes (see for example the Scenarios and Models Assessment SPM).5\nIn the context of IPBES, policy support tools and methodologies may be defined as approaches based on science and other knowledge systems that can inform, assist and enhance relevant decisions, policymaking and implementation at local, national, regional and global levels in order to protect nature, thereby promoting nature’s contributions to people and a good quality of life (IPBES core glossary).6\nAssessments may identify and assess the availability, effectiveness, practicability and replicability of current and emerging policy support tools and methodologies, as well as identify related gaps and needs7"
  },
  {
    "objectID": "Stage_2.html#steps-to-developing-an-spm",
    "href": "Stage_2.html#steps-to-developing-an-spm",
    "title": "Expert evaluation of the state of knowledge",
    "section": "7.2 Steps to developing an SPM",
    "text": "7.2 Steps to developing an SPM\nStep 1: Developing executive summaries The first step in developing an SPM is the development of an executive summary for each chapter. The executive summaries set out the key findings with the appropriate confidence terms for a particular chapter (see section 2.2.6.8 for further guidance on applying confidence terms). The content of the executive summary should be technical in nature and based on the analysis set out in the chapter.\nStep 2: Identify the policy-relevant messages One of the key differences between the executive summaries and the SPM is moving from the process of setting out technical facts to blending and synthesizing the findings from different chapters into policy-relevant messages. These messages aim to tell a short, coherent and compelling story on the state of knowledge (see Box 2.5).\nThis stage is critical for fine-tuning the articulation of key findings and policy-relevant messages in the SPM, developing graphics, quality assurance of chapters, and ensuring consistency and traceability of confidence statements between the SPM and chapters. The drafts are process‑validated by the MEP and Bureau and are presented by the secretariat to the Plenary during Stage 3 – approval and acceptance of the final assessment report.8\nThe SPM for the IPBES assessments are approved line by line within the Plenary, therefore it is important to develop a succinct summary based upon the analysis of the assessment. Use confidence terminology to ensure that no ambiguity appears with regards to the messages and analyses in the SPM. Each finding should also contain a footnote with a reference back to the number of the section or sections of the main report that the finding is drawn from.\n\n\nBox 2.5: Example (extract) of how key findings (below the dashed line) are integrated into a key message (above the dashed line)\n\n\n\nSource: The IPBES Pollinators, Pollination and Food Production Assessment- key message 12; findings from page XXXV of the SPM."
  },
  {
    "objectID": "Stage_2.html#footnotes",
    "href": "Stage_2.html#footnotes",
    "title": "Expert evaluation of the state of knowledge",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIPBES/2/17: Final report and decisions of the second session of the Plenary of IPBES. https://www.ipbes.net/document-library-catalogue/ipbes217. Decision IPBES-4/3: Rules and procedures for the operation of the platform. https://www.ipbes.net/document-library-catalogue/decision-ipbes-43↩︎\nIPBES/4/19: IPBES-4 Meeting Report. Page 107. https://www.ipbes.net/document-library-catalogue/ipbes419. Decision IPBES-4/3: Rules and procedures for the operation of the platform. Annex I. https://www.ipbes.net/document-library-catalogue/decision-ipbes-43↩︎\nDecision IPBES-4/3: Rules and prodecures for the operation of the Platform. &lt;https://www.ipbes.net/document- library-catalogue/decision-ipbes-43&gt;↩︎\nhttps://www.ipbes.net/conflict-interest-policy-implementation-procedures↩︎\nhttps://www.ipbes.net/sites/default/files/downloads/pdf/SPM_Deliverable_3c.pdf↩︎\nThe policy support tools and methodologies guidance is under development and may modify this section.↩︎\nIPBES/6/INF/16: Information on work related to policy support tools and methodologies https://www.ipbes.net/system/tdf/ipbes-6-inf-16\\_-\\_re-issued.pdf?file=1&type=node&id=16529↩︎\nValidation of the Platform’s reports is a process by which the MEP and Bureau provide their endorsement that the processes for the preparation of the Platform reports have been duly followed (IPBES/3/18, p.75).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The IPBES Guide on the production of assessments",
    "section": "",
    "text": "Add some basic info here on: - What this is - How to use it - How to contribute?\nand other introduct=ory asopects."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "The IPBES Guide on the production of assessments",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#stage-1-requests-and-scope",
    "href": "index.html#stage-1-requests-and-scope",
    "title": "The IPBES Guide on the production of assessments",
    "section": "Stage 1: Requests and scope",
    "text": "Stage 1: Requests and scope"
  },
  {
    "objectID": "index.html#stage-2-expert-evaluation-of-the-state-of-knowledge",
    "href": "index.html#stage-2-expert-evaluation-of-the-state-of-knowledge",
    "title": "The IPBES Guide on the production of assessments",
    "section": "Stage 2: Expert evaluation of the state of knowledge",
    "text": "Stage 2: Expert evaluation of the state of knowledge"
  },
  {
    "objectID": "index.html#stage-3-approvalacceptance",
    "href": "index.html#stage-3-approvalacceptance",
    "title": "The IPBES Guide on the production of assessments",
    "section": "Stage 3: Approval/acceptance",
    "text": "Stage 3: Approval/acceptance"
  },
  {
    "objectID": "index.html#stage-4-use-of-the-assessment-findings",
    "href": "index.html#stage-4-use-of-the-assessment-findings",
    "title": "The IPBES Guide on the production of assessments",
    "section": "Stage 4: Use of the assessment findings",
    "text": "Stage 4: Use of the assessment findings"
  }
]